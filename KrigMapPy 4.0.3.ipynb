{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629e947b",
   "metadata": {},
   "source": [
    "## This is the KrigMapP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8e7af",
   "metadata": {},
   "source": [
    "Import all imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ecdf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade notebook jupyterlab\n",
    "#!pip install --upgrade ipython\n",
    "#!pip install --upgrade certifi\n",
    "#%pip install -r requirements.txt\n",
    "#!pip install -r nicegui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c803fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from matplotlib.path import Path as MplPath\n",
    "import contextily as ctx\n",
    "import matplotlib.ticker as mticker\n",
    "from scipy.spatial import ConvexHull\n",
    "import skgstat as skg\n",
    "import seaborn as sns\n",
    "from pyproj import Transformer\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b254fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = [\n",
    "\"2025-06-26-Seibersdorf-PGIS2_dose.input.xlsx\"\n",
    "]\n",
    "\n",
    "#import parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "311475a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'inputs/2025-06-26-Seibersdorf_PGIS2_dose.input.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_name[\u001b[32m0\u001b[39m].endswith(\u001b[33m'\u001b[39m\u001b[33m.xlsx\u001b[39m\u001b[33m'\u001b[39m): \n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     parameters = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minputs/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msource_name\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m---READING: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_name[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m file_fields:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\backpacks\\GitHub\\KrigMapRPy\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\backpacks\\GitHub\\KrigMapRPy\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\backpacks\\GitHub\\KrigMapRPy\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\backpacks\\GitHub\\KrigMapRPy\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'inputs/2025-06-26-Seibersdorf_PGIS2_dose.input.xlsx'"
     ]
    }
   ],
   "source": [
    "\n",
    "if source_name[0].endswith('.xlsx'): \n",
    "    parameters = pd.read_excel(f\"inputs/{source_name[0]}\")\n",
    "    print(f\"---READING: {source_name[0]}\")\n",
    "\n",
    "for field in file_fields:\n",
    "    match = parameters.loc[parameters.iloc[:, 0] == field, parameters.columns[1]]\n",
    "    if not match.empty:\n",
    "        input_par[field] = str(match.iloc[0])\n",
    "        print(f\"{field}: {input_par[field]}\")\n",
    "\n",
    "for field in string_fields:\n",
    "    match = parameters.loc[parameters.iloc[:, 0] == field, parameters.columns[1]]\n",
    "    if not match.empty:\n",
    "        input_par[field] = str(match.iloc[0])\n",
    "        print(f\"{field}: {input_par[field]}\")\n",
    "\n",
    "for field in num_fields:\n",
    "    match = parameters.loc[parameters.iloc[:, 0] == field, parameters.columns[1]]\n",
    "    if not match.empty:\n",
    "        # Convert to numeric, coerce errors to NaN if non-numeric\n",
    "        input_par[field] = pd.to_numeric(match.iloc[0], errors=\"coerce\")\n",
    "        print(f\"{field}: {input_par[field]}\")\n",
    "\n",
    "if input_par[\"filename\"].endswith('.csv'): data = pd.read_csv(f\"data/{input_par['filename']}\")\n",
    "if input_par[\"filename\"].endswith('.xlsx'): data = pd.read_excel(f\"data/{input_par['filename']}\")\n",
    "\n",
    "# Rename columns dynamically using input mapping\n",
    "data = data.rename(columns={\n",
    "    input_par[\"longitude\"]: \"lon\",\n",
    "    input_par[\"latitude\"]: \"lat\",\n",
    "    input_par[\"column\"]: \"value\"\n",
    "})\n",
    "data = data[[\"lon\", \"lat\", \"value\"]] # Select only the required columns\n",
    "data[\"value\"] = data[\"value\"]*input_par[\"constant\"]\n",
    "data[\"dataset\"] = input_par[\"detector\"]\n",
    "data_trimmed = data.iloc[input_par[\"trimmBeginningUpTo\"]:input_par[\"trimmEndAfter\"]]\n",
    "detectorname = input_par[\"detector\"] \n",
    "\n",
    "\n",
    "if len(source_name) > 1:\n",
    "    for src in range(1, len(source_name)):\n",
    "        input_par[\"trimmBeginningUpTo\"]= 0\n",
    "        input_par[\"trimmEndAfter\"] = 1000000\n",
    "        if source_name[src].endswith('.xlsx'): \n",
    "            parameters = pd.read_excel(f\"inputs/{source_name[src]}\")\n",
    "            print(f\"---READING: {source_name[src]}\")\n",
    "        for field in file_fields:\n",
    "            match = parameters.loc[parameters.iloc[:, 0] == field, parameters.columns[1]]\n",
    "            if not match.empty:\n",
    "                input_par[field] = str(match.iloc[0])\n",
    "                print(f\"{field}: {input_par[field]}\")\n",
    "        for field in string_fields:\n",
    "            match = parameters.loc[parameters.iloc[:, 0] == field, parameters.columns[1]]\n",
    "            if not match.empty:\n",
    "                input_par[field] = str(match.iloc[0])\n",
    "                print(f\"{field}: {input_par[field]}\")\n",
    "        for field in num_fields:\n",
    "            match = parameters.loc[parameters.iloc[:, 0] == field, parameters.columns[1]]\n",
    "            if not match.empty:\n",
    "                # Convert to numeric, coerce errors to NaN if non-numeric\n",
    "                input_par[field] = pd.to_numeric(match.iloc[0], errors=\"coerce\")\n",
    "                print(f\"{field}: {input_par[field]}\")\n",
    "        if input_par[\"filename\"].endswith('.csv'): data_temp = pd.read_csv(f\"data/{input_par[\"filename\"]}\")\n",
    "        if input_par[\"filename\"].endswith('.xlsx'): data_temp = pd.read_excel(f\"data/{input_par[\"filename\"]}\")\n",
    "        # Rename columns dynamically using input mapping\n",
    "        data_temp = data_temp.rename(columns={\n",
    "            input_par[\"longitude\"]: \"lon\",\n",
    "            input_par[\"latitude\"]: \"lat\",\n",
    "            input_par[\"column\"]: \"value\"\n",
    "        })\n",
    "        data_temp = data_temp[[\"lon\", \"lat\", \"value\"]] # Select only the required columns\n",
    "        data_temp[\"value\"] = data_temp[\"value\"]*input_par[\"constant\"]\n",
    "        data_temp[\"dataset\"] = input_par[\"detector\"]\n",
    "        data = pd.concat([data, data_temp], ignore_index=True)\n",
    "        data_trimmed = pd.concat([data_trimmed, data_temp.iloc[input_par[\"trimmBeginningUpTo\"]:input_par[\"trimmEndAfter\"]]], ignore_index=True)\n",
    "        detectorname = f\"{detectorname}+{input_par[\"detector\"]}\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = sns.displot(\n",
    "    data=data,\n",
    "    x=\"value\",\n",
    "    row=\"dataset\",     # facet by dataset (rows)\n",
    "    bins=30,\n",
    "    color=\"skyblue\",\n",
    "    edgecolor=\"black\",\n",
    "    height=3, aspect=3\n",
    ")\n",
    "g.set_axis_labels(input_par[\"quantity\"], \"Frequency\")\n",
    "g.set_titles(row_template=\"{row_name}\")   # show dataset name on each plot\n",
    "plt.show()\n",
    "\n",
    "# Add an original row index to preserve order\n",
    "data = data.reset_index(drop=False).rename(columns={\"index\": \"original_row\"})\n",
    "# Function to assign groups and ranges safely\n",
    "def ntile_by_order(df, n_groups=5):\n",
    "    df = df.copy()\n",
    "    n = len(df)\n",
    "    # create a new \"row_in_dataset\" starting from 1 for each dataset\n",
    "    df['row_in_dataset'] = np.arange(1, n+1)\n",
    "    # assign group number\n",
    "    df['group'] = np.ceil(df['row_in_dataset'] * n_groups / n).astype(int)    \n",
    "    # create row range labels for each group\n",
    "    group_ranges = df.groupby('group')['row_in_dataset'].agg(['min', 'max'])\n",
    "    df['label'] = df['group'].map(lambda g: f\"{group_ranges.loc[g,'min']}-{group_ranges.loc[g,'max']}\")\n",
    "    return df\n",
    "#\n",
    "# Apply grouping\n",
    "data_new = data.groupby(\"dataset\", group_keys=False).apply(ntile_by_order, n_groups=5)\n",
    "# Create FacetGrid\n",
    "g = sns.FacetGrid(\n",
    "    data_new, \n",
    "    row=\"dataset\", col=\"group\", \n",
    "    height=3, aspect=1.2,\n",
    "    sharex=True, sharey=True\n",
    ")\n",
    "g.map_dataframe(\n",
    "    sns.scatterplot,\n",
    "    x=\"lon\", y=\"lat\",\n",
    "    hue=\"value\",\n",
    "    palette=\"viridis\",\n",
    "    alpha=0.8,\n",
    "    s=10\n",
    ")\n",
    "# Fix aspect ratio\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "#\n",
    "# Legend + axis labels\n",
    "g.add_legend(title=input_par[\"quantity\"])\n",
    "g.set_axis_labels(\"Longitude\", \"Latitude\")\n",
    "# Set titles with dataset name + row range\n",
    "for (dataset, group), ax in g.axes_dict.items():\n",
    "    label = data_new.loc[\n",
    "        (data_new[\"dataset\"]==dataset) & (data_new[\"group\"]==group),\n",
    "        \"label\"\n",
    "    ].iloc[0]\n",
    "    ax.set_title(f\"{dataset}, {label}\")\n",
    "plt.show()\n",
    "\n",
    "data = data_trimmed\n",
    "\n",
    "# Facet by dataset (stacked vertically)\n",
    "g = sns.FacetGrid(data, row=\"dataset\", height=4, aspect=1.5, sharex=True, sharey=True)\n",
    "# Scatter plot with color mapped to \"value\"\n",
    "g.map_dataframe(\n",
    "    sns.scatterplot,\n",
    "    x=\"lon\", y=\"lat\",\n",
    "    hue=\"value\",\n",
    "    palette=\"viridis\",\n",
    "    alpha=0.8,\n",
    "    s=10   # size of points\n",
    ")\n",
    "# Adjust legend and labels\n",
    "g.add_legend(title=input_par[\"quantity\"])\n",
    "g.set_axis_labels(\"Longitude\", \"Latitude\")\n",
    "g.set_titles(row_template=\"{row_name}\")   # dataset name as facet title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c86d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utm = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data[\"lon\"], data[\"lat\"]), crs=\"EPSG:\" + str(crs))\n",
    "data_utm = data_utm.to_crs(epsg=input_par[\"utm\"])\n",
    "\n",
    "# Create an array of UTM coordinates\n",
    "utm_coords = np.column_stack((data_utm.geometry.x, data_utm.geometry.y))\n",
    "# Extract the results from the unique GeoDataFrame\n",
    "values = data_utm[\"value\"].values\n",
    "\n",
    "################\n",
    "# Dimensions for interpolation and later drawings - x,y in meters and lon, lat in degrees\n",
    "# Precompute min, max, and midpoints\n",
    "x_min_val, x_max_val = utm_coords[:, 0].min(), utm_coords[:, 0].max()\n",
    "y_min_val, y_max_val = utm_coords[:, 1].min(), utm_coords[:, 1].max()\n",
    "\n",
    "transformer = Transformer.from_crs(\"EPSG:\" + str(input_par[\"utm\"]), \"EPSG:\" + str(crs), always_xy=True)\n",
    "lon_min, lat_min = transformer.transform(x_min_val, y_min_val)\n",
    "lon_max, lat_max = transformer.transform(x_max_val, y_max_val)\n",
    "\n",
    "# Correct aspect ratio for lon/lat\n",
    "lat_mid = 0.5 * (lat_min + lat_max)\n",
    "ratio = 1 / np.cos(np.radians(lat_mid))\n",
    "\n",
    "# Compute number of cells in x and y\n",
    "x_width = x_max_val - x_min_val\n",
    "y_width = y_max_val - y_min_val\n",
    "width = max(x_width, y_width)\n",
    "x_mid = (x_max_val + x_min_val) / 2.0\n",
    "y_mid = (y_max_val + y_min_val) / 2.0\n",
    "\n",
    "# Apply scaling for the figure later\n",
    "x_min_fig = x_mid - 0.7 * width\n",
    "x_max_fig = x_mid + 0.7 * width\n",
    "y_min_fig = y_mid - 0.7 * width\n",
    "y_max_fig = y_mid + 0.7 * width\n",
    "\n",
    "# Grid for interpolation\n",
    "xgrid, ygrid = np.meshgrid(np.linspace(x_min_val, x_max_val, int(x_width / input_par[\"resolution\"])), \n",
    "                           np.linspace(y_min_val, y_max_val, int(y_width / input_par[\"resolution\"])))\n",
    "# Create the convex hull and a Path object from the hull vertices\n",
    "hull_path = MplPath(utm_coords[ConvexHull(utm_coords).vertices])\n",
    "# Mask out areas outside the convex hull to avoid extrapolation\n",
    "mask = hull_path.contains_points(np.c_[xgrid.ravel(), ygrid.ravel()])\n",
    "mask = mask.reshape(xgrid.shape)\n",
    "\n",
    "# Convert grids back to the original coordinate reference system for mapping\n",
    "\n",
    "lon_min_fig, lat_min_fig = transformer.transform(x_min_fig, y_min_fig)\n",
    "lon_max_fig, lat_max_fig = transformer.transform(x_max_fig, y_max_fig)\n",
    "lon_grid, lat_grid = transformer.transform(xgrid, ygrid)  # both remain 2D arrays matching xgrid/ygrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669fd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_bins=np.arange(1, 10, 1)\n",
    "\n",
    "# Create a Variogram object and fit it to the data using custom bin boundaries\n",
    "# https://scikit-gstat.readthedocs.io/en/latest/reference/variogram.html here are the documentation to tell you \n",
    "                                                                        #what are the variogram parameters\n",
    "variogram = skg.Variogram(\n",
    "    coordinates= utm_coords, \n",
    "    values= values,\n",
    "    estimator='matheron',\n",
    "    model='spherical',\n",
    "    bin_func='even', #so this is saying that the bins will have the same width which is default 1, can be modified \n",
    "    #bin_edges=custom_bins ,                 #https://scikit-gstat.readthedocs.io/en/latest/reference/binning.html\n",
    "    normalize=False,\n",
    "    use_nugget=True,\n",
    "    fit_method='trf',\n",
    "    #fit_sigma='linear',\n",
    "    #fit_range=10, ##so comment out these range, sill, nugget values if you want,\n",
    "    #fit_sill=22.79,\n",
    "    fit_nugget=1.0,\n",
    "    n_lags=40\n",
    "    # ,  \n",
    "    #maxlag=40 \n",
    ")\n",
    "\n",
    "print(variogram)\n",
    "\n",
    "# Plot the variogram\n",
    "plt.figure(figsize=(5, 3))\n",
    "variogram.plot(show=True)\n",
    "plt.show()\n",
    "\n",
    "variogram = skg.Variogram(\n",
    "    coordinates= utm_coords, \n",
    "    values= values,\n",
    "    estimator='matheron',\n",
    "    model='spherical',\n",
    "    bin_func='even', #so this is saying that the bins will have the same width which is default 1, can be modified \n",
    "    #bin_edges=custom_bins ,                 #https://scikit-gstat.readthedocs.io/en/latest/reference/binning.html\n",
    "    normalize=False,\n",
    "    use_nugget=True,\n",
    "    fit_method='trf',\n",
    "    #fit_sigma='linear',\n",
    "    #fit_range=10, ##so comment out these range, sill, nugget values if you want,\n",
    "    #fit_sill=22.79,\n",
    "    fit_nugget=1.0,\n",
    "    n_lags=40,  \n",
    "    maxlag=input_par[\"variogramMax\"]\n",
    ")\n",
    "\n",
    "print(variogram)\n",
    "\n",
    "# Plot the variogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "variogram.plot(show=True)\n",
    "plt.show()\n",
    "\n",
    "# Set up Ordinary Kriging with predefined variogram model and control over the number of points used for interpolation\n",
    "OK = skg.OrdinaryKriging(\n",
    "    variogram,  # variogram model calculated previously\n",
    "    min_points=1,  # minimum number of points to include in kriging calculation\n",
    "    max_points=100,\n",
    "    # mode='exact'  # maximum number of points to include in kriging calculation\n",
    ")\n",
    "\n",
    "# Perform Kriging interpolation\n",
    "OK.transform(xgrid.ravel(), ygrid.ravel()) \n",
    "# ravel() flattens the grid into 1D arrays to feed them into the kriging function.\n",
    "# Reshape the standard deviations to match the original grid shape to put them back into a 2D raster\n",
    "Z_sigma = OK.sigma.reshape(xgrid.shape)\n",
    "Z_pred = OK.z.reshape(xgrid.shape)\n",
    "Z_sigma = (np.sqrt(Z_sigma) / Z_pred) * 100\n",
    "\n",
    "Z_pred = np.where(mask, Z_pred, np.nan)  # Apply mask to the interpolated data\n",
    "Z_sigma = np.where(mask, Z_sigma, np.nan)  # Apply the mask to the standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb885487",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0, axs0 = plt.subplots()  \n",
    "\n",
    "# Set axis limits with padding\n",
    "axs0.set_xlim(lon_min_fig, lon_max_fig)\n",
    "axs0.set_ylim(lat_min_fig, lat_max_fig)\n",
    "\n",
    "# basemap for better geographic context\n",
    "ctx.add_basemap(axs0, crs=\"EPSG:\" + str(crs), source=ctx.providers.Esri.WorldImagery)\n",
    "                                        # ctx.providers.Esri.WorldImagery,\n",
    "                                        # ctx.providers.OpenStreetMap.Mapnik,\n",
    "                                        # ctx.providers.Esri.DeLorme,\n",
    "                                        # ctx.providers.CartoDB.Positron,\n",
    "                                        # ctx.providers.CartoDB.DarkMatter,\n",
    "                                        # ctx.providers.OpenTopoMap,\n",
    "\n",
    "sc = axs0.scatter(\n",
    "    data[\"lon\"], data[\"lat\"],       # x and y\n",
    "    c=data[\"value\"],                # colors\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.5,\n",
    "    edgecolor=\"none\",\n",
    "    s=20\n",
    ")\n",
    "plt.colorbar(sc, ax=axs0, label='nGy/h')\n",
    "\n",
    "# Format axis to display longitude and latitude in plain numbers\n",
    "axs0.xaxis.set_major_formatter(mticker.ScalarFormatter(useOffset=False))\n",
    "plt.setp(axs0.get_xticklabels(), rotation=-25, ha='center')\n",
    "axs0.yaxis.set_major_formatter(mticker.ScalarFormatter(useOffset=False))\n",
    "axs0.set(xlabel='Longitude', ylabel='Latitude', title='Kriging Interpolation with Basemap')\n",
    "\n",
    "# Save to a file\n",
    "plt.savefig(f\"printouts/{input_par[\"site\"]}-{detectorname}-{input_par[\"unitName\"]}-survey.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "# plt.savefig(\"myplot.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "######################################################\n",
    "\n",
    "# plot to visualise the interpolated data\n",
    "fig1, axs1 = plt.subplots()  \n",
    "\n",
    "# Set axis limits with padding\n",
    "axs1.set_xlim(lon_min_fig, lon_max_fig)\n",
    "axs1.set_ylim(lat_min_fig, lat_max_fig)\n",
    "\n",
    "# basemap for better geographic context\n",
    "ctx.add_basemap(axs1, crs=\"EPSG:\" + str(crs), source=ctx.providers.Esri.WorldImagery)\n",
    "                                        # ctx.providers.Esri.WorldImagery,\n",
    "                                        # ctx.providers.OpenStreetMap.Mapnik,\n",
    "                                        # ctx.providers.Esri.DeLorme,\n",
    "                                        # ctx.providers.CartoDB.Positron,\n",
    "                                        # ctx.providers.CartoDB.DarkMatter,\n",
    "                                        # ctx.providers.OpenTopoMap,\n",
    "\n",
    "im = axs1.imshow(\n",
    "    Z_pred,\n",
    "    origin='lower',\n",
    "    extent=(lon_grid.min(), lon_grid.max(), lat_grid.min(), lat_grid.max()),\n",
    "    aspect= ratio,\n",
    "    cmap='viridis',\n",
    "    interpolation='none',  # show actual values per cell,\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.colorbar(im, ax=axs1, label='nGy/h')\n",
    "\n",
    "# Format axis to display longitude and latitude in plain numbers\n",
    "axs1.xaxis.set_major_formatter(mticker.ScalarFormatter(useOffset=False))\n",
    "plt.setp(axs1.get_xticklabels(), rotation=-25, ha='center')\n",
    "axs1.yaxis.set_major_formatter(mticker.ScalarFormatter(useOffset=False))\n",
    "axs1.set(xlabel='Longitude', ylabel='Latitude', title=f\"{input_par[\"site\"]} {detectorname} kriging\")\n",
    "\n",
    "# Save to a file\n",
    "plt.savefig(f\"printouts/{input_par[\"site\"]}-{detectorname}-{input_par[\"unitName\"]}-krig.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "# plt.savefig(\"myplot.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# plot to visualise the interpolated data\n",
    "fig2, axs2 = plt.subplots()  # Create a subplot with 2 columns\n",
    "\n",
    "axs2.set_xlim(lon_min_fig, lon_max_fig)\n",
    "axs2.set_ylim(lat_min_fig, lat_max_fig)\n",
    "\n",
    "# basemap for better geographic context\n",
    "ctx.add_basemap(axs2, crs=\"EPSG:\" + str(crs), source=ctx.providers.Esri.WorldImagery)\n",
    "\n",
    "im = axs2.imshow(\n",
    "    Z_sigma,\n",
    "    origin='lower',\n",
    "    extent=(lon_grid.min(), lon_grid.max(), lat_grid.min(), lat_grid.max()),\n",
    "    aspect= ratio,\n",
    "    cmap='viridis',\n",
    "    interpolation='none',  # show actual values per cell,\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.colorbar(im, ax=axs2, label='Uncertainty [%]')\n",
    "\n",
    "# Format axis to display longitude and latitude in plain numbers\n",
    "axs2.xaxis.set_major_formatter(mticker.ScalarFormatter(useOffset=False))\n",
    "plt.setp(axs2.get_xticklabels(), rotation=-25, ha='center')\n",
    "axs2.yaxis.set_major_formatter(mticker.ScalarFormatter(useOffset=False))\n",
    "\n",
    "axs2.set_xlabel('Longitude')\n",
    "axs2.set_ylabel('Latitude')\n",
    "axs2.set_title('Kriging Standard Deviations with Basemap')\n",
    "# Save to a file\n",
    "plt.savefig(f\"printouts/{input_par[\"site\"]}-{detectorname}-{input_par[\"unitName\"]}-krig_unc.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "\n",
    "# Flip vertically so north is up\n",
    "Z_export = np.flipud(Z_pred)\n",
    "# Define pixel size (degrees) from grid spacing\n",
    "xres = (lon_grid.max() - lon_grid.min()) / (lon_grid.shape[1] - 1)\n",
    "yres = (lat_grid.max() - lat_grid.min()) / (lat_grid.shape[0] - 1)\n",
    "\n",
    "# Affine transform for raster\n",
    "transform = from_origin(lon_grid.min(), lat_grid.max(), xres, yres)\n",
    "\n",
    "# Save GeoTIFF\n",
    "with rasterio.open(\n",
    "    \"rasters/raster-krig.tif\",\n",
    "    \"w\",\n",
    "    driver=\"GTiff\",\n",
    "    height=Z_export.shape[0],\n",
    "    width=Z_export.shape[1],\n",
    "    count=1,\n",
    "    dtype=Z_export.dtype,\n",
    "    crs=\"EPSG:\" + str(crs),  # WGS84\n",
    "    transform=transform\n",
    "    #,\n",
    "    #nodata=-9999\n",
    ") as dst:\n",
    "    dst.write(Z_export, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
